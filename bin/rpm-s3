#!/usr/bin/env python
"""CLI for serialising metadata updates on an s3-hosted yum repository.
"""
import os
import sys
import time
import urlparse
import tempfile
import shutil
import optparse
import logging
import collections
import yum
import boto3
import botocore
import subprocess

lib_root = os.path.dirname(os.path.dirname(__file__))

sys.path.insert(1, os.path.join(lib_root, "vendor/pexpect"))
import pexpect

sys.path.insert(1, os.path.join(lib_root, "vendor/createrepo"))
import createrepo

class LoggerCallback(object):
    def errorlog(self, message):
        logging.error(message)

    def log(self, message):
        message = message.strip()
        if message:
            logging.debug(message)


class S3Grabber(object):
    def __init__(self, baseurl, visibility, host):
        logging.info('S3Grabber: %s', baseurl)
        base = urlparse.urlsplit(baseurl)
        self.baseurl = baseurl
        self.basepath = base.path.lstrip('/')
        self.s3 = get_s3(host)
        self.bucket = self.s3.Bucket(base.netloc)
        self.visibility = visibility

    def check(self, url):
        if url.startswith(self.baseurl):
            url = url[len(self.baseurl):].lstrip('/')

        logging.info("checking if key exists: %s", os.path.join(self.basepath, url))

        key = os.path.join(self.basepath, url)

        try:
            obj = self.bucket.Object(key)
            obj.get()
            return obj
        except botocore.exceptions.ClientError as e:
            if 'NoSuchKey' in e.message:
                return None
            else:
                raise e

    def urlgrab(self, url, filename, **kwargs):
        logging.info('urlgrab: %s', filename)

        obj = self.check(url)
        if not obj:
            raise createrepo.grabber.URLGrabError(14, '%s not found' % url)

        logging.info('downloading: %s', obj.key)

        self.bucket.download_file(obj.key, filename)

        return filename

    def syncdir(self, dir, url):
        """Copy all files in dir to url, removing any existing keys."""
        base = os.path.join(self.basepath, url)
        existing_objects = list(self.bucket.objects.filter(Prefix=base))
        new_objects = []

        for filename in sorted(os.listdir(dir)):
            source = os.path.join(dir, filename)
            target = os.path.join(base, filename)
            obj = self.bucket.Object(target)

            with open(source, 'rb') as data:
                obj.upload_fileobj(data)

            obj.Acl().put(ACL=self.visibility)

            new_objects.append(obj.key)
            logging.info('uploading: %s from: %s', obj.key, source)

        for obj in existing_objects:
            if obj.key not in new_objects:
                obj.delete()
                logging.info('removing: %s', obj.key)

    def upload(self, file, url):
        """Copy file to url."""
        target = os.path.join(self.basepath, url)
        logging.info("upload: %s to %s", (file, target))

        obj = self.bucket.Object(target)

        with open(file, 'rb') as data:
                obj.upload_fileobj(data)

        obj.Acl().put(ACL=self.visibility)


class FileGrabber(object):
    def __init__(self, baseurl):
        logging.info('FileGrabber: %s', baseurl)
        self.basepath = urlparse.urlsplit(baseurl).path
        logging.info('basepath: %s', self.basepath)

    def urlgrab(self, url, filename, **kwargs):
        """Expects url of the form: file:///path/to/file"""
        filepath = urlparse.urlsplit(url).path
        realfilename = os.path.join(self.basepath, filepath)
        logging.info('fetching: %s', realfilename)
        # The filename name is REALLY important for createrepo as it derives
        # the base path from its own tempdir, etc.
        os.symlink(realfilename, filename)
        return filename


def get_s3(host_url):
    session = boto3.session.Session()
    return session.resource('s3', endpoint_url=host_url)

def sign(rpmfile):
    """Requires a proper ~/.rpmmacros file. See <http://fedoranews.org/tchung/gpg/>"""
    # TODO: check if file is indeed signed
    cmd = "rpm --resign '%s'" % rpmfile
    logging.info(cmd)
    try:
        child = pexpect.spawn(cmd)
        child.expect('Enter pass phrase: ')
        child.sendline('')
        child.expect(pexpect.EOF)
    except pexpect.EOF, e:
        print "Unable to sign package '%s' - %s" % (rpmfile, child.before)
        logging.error("Unable to sign package: %s", e)
        exit(1)

def sign_metadata(repomdfile):
    """Requires a proper ~/.rpmmacros file. See <http://fedoranews.org/tchung/gpg/>"""
    cmd = ["gpg", "--detach-sign", "--armor", repomdfile]
    logging.info(cmd)
    try:
        subprocess.check_call(cmd)
        logging.info("Successfully signed repository metadata file")
    except subprocess.CalledProcessError, e:
        print "Unable to sign repository metadata '%s'" % (repomdfile)
        logging.error("Unable to sign repository metadata: %s", e)
        exit(1)

def setup_repository(repo, repopath):
    """Make sure a repo is present at repopath"""
    key = repo._grab.check("repodata/repomd.xml")
    if key:
        logging.info("Existing repository detected.")
    else:
        path_to_empty_repo = os.path.join(lib_root, "empty-repo", "repodata")
        logging.info("Empty repository detected. Initializing with empty repodata...")
        repo._grab.syncdir(path_to_empty_repo, "repodata")

def update_repodata(repopath, rpmfiles, options):
    logging.info('rpmfiles: %s', rpmfiles)
    tmpdir = tempfile.mkdtemp()
    s3base = urlparse.urlunsplit(('s3', options.bucket, repopath, '', ''))
    s3grabber = S3Grabber(s3base, options.visibility, options.host)
    filegrabber = FileGrabber("file://" + os.getcwd())

    # Set up temporary repo that will fetch repodata from s3
    yumbase = yum.YumBase()
    yumbase.preconf.disabled_plugins = '*'
    yumbase.conf.cachedir = os.path.join(tmpdir, 'cache')
    yumbase.repos.disableRepo('*')
    repo = yumbase.add_enable_repo('s3')
    repo._grab = s3grabber

    # Add all needed architectures to the repo list
    for arch in options.archs:
        yumbase.arch.archlist.append(arch)

    setup_repository(repo, repopath)

    # Ensure that missing base path doesn't cause trouble
    repo._sack = yum.sqlitesack.YumSqlitePackageSack(
        createrepo.readMetadata.CreaterepoPkgOld)

    # Create metadata generator
    mdconf = createrepo.MetaDataConfig()
    mdconf.sumtype = options.sumtype
    mdconf.directory = tmpdir
    mdgen = createrepo.MetaDataGenerator(mdconf, LoggerCallback())
    mdgen.tempdir = tmpdir

    # Log old package list
    logging.info("Previous package list: %s", list(yumbase.pkgSack))

    # Combine existing package sack with new rpm file list
    new_packages = []
    for rpmfile in rpmfiles:
        rpmfile = os.path.realpath(rpmfile)
        logging.info("rpmfile: %s", rpmfile)

        if options.sign:
            sign(rpmfile)

        mdgen._grabber = filegrabber
        # please, don't mess with my path in the <location> tags of primary.xml.gz
        relative_path = "."
        newpkg = mdgen.read_in_package("file://" + rpmfile, relative_path)
        mdgen._grabber = s3grabber

        # don't put a base url in <location> tags of primary.xml.gz
        newpkg._baseurl = None
        older_pkgs = yumbase.pkgSack.searchNevra(name=newpkg.name)

        # Remove older versions of this package (or if it's the same version)
        for older in older_pkgs:
            if older.pkgtup == newpkg.pkgtup:
                yumbase.pkgSack.delPackage(older)
                logging.info('ignoring: %s', older.ui_nevra)

        # The package is now removed if it has the same name
        # If we passed -d, then we don't want to add it back
        # if we didn't then we wnat to include the package
        if not options.delete:
            new_packages.append(newpkg)

    mdconf.pkglist = list(yumbase.pkgSack) + new_packages

    # Log new package list
    logging.info("New package list: %s", mdconf.pkglist)

    # Write out new metadata to tmpdir
    mdgen.doPkgMetadata()
    mdgen.doRepoMetadata()
    mdgen.doFinalMove()

    # Upload rpm files to destination
    for rpmfile in rpmfiles:
        rpmfile = os.path.realpath(rpmfile)
        logging.info("rpmfile: %s", rpmfile)
        s3grabber.upload(rpmfile, os.path.basename(rpmfile))


    # Generate repodata/repomd.xml.asc
    if options.sign:
        sign_metadata(os.path.join(tmpdir, 'repodata', 'repomd.xml'))

    # Replace metadata on s3
    s3grabber.syncdir(os.path.join(tmpdir, 'repodata'), 'repodata')

    shutil.rmtree(tmpdir)

def main(options, args):
    loglevel = ('WARNING', 'INFO', 'DEBUG')[min(2, options.verbose)]
    logging.basicConfig(
        filename=options.logfile,
        level=logging.getLevelName(loglevel),
        format='%(asctime)s %(levelname)s %(message)s',
    )

    update_repodata(options.repopath, args, options)

def get_comma_separated_args(option, opt, value, parser):
    setattr(parser.values, option.dest, value.split(','))

if __name__ == '__main__':
    parser = optparse.OptionParser()
    parser.add_option('-b', '--bucket', default='my-bucket')
    parser.add_option('-p', '--repopath', default='')
    parser.add_option('-v', '--verbose', action='count', default=0)
    parser.add_option('--visibility', default='private')
    parser.add_option('-s', '--sign', action='count', default=0)
    parser.add_option('-l', '--logfile')
    parser.add_option('-d', '--delete', action='store_true', default=False)
    parser.add_option('-r', '--region')
    parser.add_option('-c', '--host', default='s3-eu-central-1.amazonaws.com')
    parser.add_option('-a', '--archs', type='string', default=[], action='callback', callback=get_comma_separated_args)
    parser.add_option('-t', '--sumtype', default='sha256')
    options, args = parser.parse_args()
    if options.region is not None and options.host != 's3-eu-central-1.amazonaws.com':
        parser.error('region and host are mutually exclusive')
    elif options.region is not None:
        options.host = "s3.amazonaws.com" if options.region == "us-east-1" else "s3-{}.amazonaws.com".format(options.region)
    main(options, args)
